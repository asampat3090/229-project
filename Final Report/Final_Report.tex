%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2012 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2012,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:

\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
\usepackage[caption=false]{subfig}

%\usepackage{epsfig} % less modern
%\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% For font
\usepackage{amssymb}

% For math equations
\usepackage{mathtools}

% For references
%\usepackage{nature}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2012} with
% \usepackage[nohyperref]{icml2012} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
%\usepackage{icml2012} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
 \usepackage[accepted]{icml2012}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{2D Visualization of Immune System Cellular Protein Data by Nonlinear Dimensionality Reduction}

\begin{document} 

\twocolumn[
\icmltitle{2D Visualization of Immune System Cellular Protein Data by Nonlinear Dimensionality Reduction}

% Author Information
\icmlauthor{Andre Esteva}{esteva@stanford.edu}
\icmladdress{Stanford University, Electrical Engineering, 496 Lomita Mall, Durand 196, Stanford, CA 94305 USA}            

\icmlauthor{Anand Sampat}{asampat@stanford.edu}
\icmladdress{Stanford University, Electrical Engineering, 450 Serra Mall, Stanford, CA 94305 USA}

\icmlauthor{Amit Badlani}{abadlani@stanford.edu}
\icmladdress{Stanford University, Electrical Engineering, 450 Serra Mall, Stanford, CA 94305 USA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{dimensionality reduction, cancer data, stochastic neighbor embedding (SNE), machine learning, CS229}

\vskip 0.3in
]

% ABSTRACT
\begin{abstract} 
We present in this paper a way to effectively visualize multi-dimensional immune system cellular data by means of nonlinear methods. We find that Stochastic Neighbor Embedding (SNE), and it's variations, t-SNE and s-SNE, to be most effective at successfully mapping clusters of points into a two dimensional embedding space while preserving both the structure between similar points and the disparity between different clusters. Using a centroid-based metric that relabels points according to the cluster centroid to which they are closest, we conclude that SNE works significantly better than linear and spectral methods. By using an optimization approach for SNE that follows the 'spectral direction' of descent, we are able to run the SNE varients and EE two orders of magnitude faster than with standard optimization. Finally, we demonstrate superior classification of data by first reducing its dimension then applying supervised learning using a C-SVC SVM and $\nu$-SVC SVM. 

\end{abstract} 

% INTRO
\section{Introduction}
\label{intro}

\subsection{Immune Cell Data}

In the field of cancer immunology, scientists use the protein content of immune system cells as a way to identify a cells corresponding type \cite{Bendall:2011bm}. For example, immune system cells, which are contained in bone marrow, are comprised of a variety of cell types, and to a large degree are uniquely identifiable by the proteins they contain. Highly sophisticated methods have been developed that process cells and return information on the types and quantities of proteins expressed in those cells. This data can then be viewed by an expert in the field and categorized. The laborious process of viewing the different dimensions of protein expression and categorizing a cell's type is known as gating. 

%Figure ~\ref{CellGating}, below, taken from \cite{Amir:2013jp} shows this graphically.
%
%\begin{figure}[ht]
%\label{CellGating}
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=\columnwidth]{CellGating}}
%\caption{Strategy for cell gating: Two single dimensions of a cell are viewed at a time
%and through an iterative process the cell is classified}
%\end{center}
%\vskip -0.2in
%\end{figure} 

\subsection{Project Goals and Metrics}

It is of interest to cancer immunologists to find structure within multi-dimensional protein expression space and map it onto a lower dimensions (refered to henceforth as a map space) for ease of visualization and understanding. As cells change and evolve, so do the types and quantities of the proteins they express. This leads to a shifting of their representation in multi-dimensional space which can be tracked. Dimensionality reduction of original biological data coupled with a metric for how well the projection represents the original data would provide biologists with a powerful tool for understanding the structure of their data. To address these challenges we demonstrate:

\begin{itemize}
\item The application of linear and non-linear methods of dimensionality reduction of multi-dimensional protein data
\item A metric-based comparison of how each algorithm performs
\end{itemize}

% DATA REPRESENTATION
\section{Data Representation}
\label{data_representation}

\subsection{Data Acquisition}

\textbf{Mass cytometry} is a single-cell multiparametric protein detection technology based on inductively coupled plasma mass spectrometry. It is an extension of flow cytometry in which antibodies are tagged with isotopically pure rare earth elements allowing simultaneous measurement of greater than 40 parameters while circumventing the issue of spectral overlap. In single-cell droplet form, the cells are passed through an elemental mass spectrometer and an integrator to generate an $m \times p$ matrix where $m$ is the number of cells processed and $p$ is the number of distinct proteins contained in the cell set. These matrices are stored as .FCS files in online databanks which we have been granted access to. Figure ~\ref{dataacquisition}, taken from \cite{Bendall:2011bm} shows this process. Table ~\ref{table_cell_type} lists the cell types and subtypes that are parsed using this method.

\begin{figure}
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{DataAcquisition}}
\caption{Overview of data acquisition, from extraction of cellular protein counts to storage in online databanks}
\end{center}
\label{dataacquisition}
\vskip -0.2in
\end{figure}  

\begin{table}
\caption{Classification of Cell Types and their Corresponding Sub-Types}
\label{table_cell_type}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\hline
\abovespace\belowspace
Cell Types & Sub-types  \\
\hline
\abovespace
Stem Cell	&  HSC, MPP, CMP, GMP, MEP \\
B Cells 	& Plasma, pre-B-I, pre-B- II, Immature, \\ 
		&			Mature CD38 low, Mature CD38 mid\\
T Cells	& Mature CD4+, Mature CD8+ \\
		& Naive CD4+, Naive CD8+  \\
NK		& -   \\
pDC		& - \\
\belowspace
Monocytes & CD11B - , CD11B high, CD11B mid \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Feature Selection}
The $p=41$ types of proteins collected for each cell using mass cytometry are comprised of both intracellular and surface proteins. These two types play fundamentally different roles in cell identification. Surface proteins are semi-permanent markers that last for significant periods of time relative to the lifetime of a cell, whereas intracellular proteins are highly transient and can change quickly. This is analogous to classifying a person based on where they live (semi-permanent) versus what they wore on a particular day (transient). Understanding this, we select as our feature space the $n=17$ surface protein markers of the cell data.

% METHODS
\section{Methods}
\label{methods}

The datafiles provided contain cell counts on the order of tens of thousands where we consider each cell to be a point in $\mathbb{R}^n$. To simplify our algorithms and account for matrix size differences in the different .FCS files we run our algorithms on equally sized portions of different cell data. In particular, if we let $\mathbb{S}$ be the set of all cell sub-types as defined in Table ~\ref{table_cell_type}, $S \in \mathbb{S}$  be some subset of interest with cardinality $|S|$, and N some fixed positive integer, then by taking N rows from each sub-type $s \in S$ we form a matrix $M \in \mathbb{R}^{N |S| \times n}$. This allows us to run algorithms that accomplish dimensionality reduction from $\mathbb{R}^n$ onto $\mathbb{R}^2$ quickly and without giving unfair weighting to a particular cell sub-type.

%We consider various algorithms which project sets of data in $\mathbb{R}^n$ onto $\mathbb{R}^2$ for easy visualization.

\subsection{Linear Methods}
Linear methods such as Principal Components Analysis (PCA) and Multidimensional Scaling are straightforward and standard ways of achieving dimensionality reduction. The caveat is the requirement that the data be linearly separable in the space being considered. Unlike SVMs, which have the ability to project data to higher dimensions in order to linearly separate it, PCA runs in the original dimension of the data. Given the nature of this data and the fact that cellular protein counts can vary largely between cell types, PCA is not an optimal method for visualizing these sorts of problems. We apply PCA and demonstrate that its visualization is quite poor, as expected.

\subsection{Spectral Methods}
Spectral methods like Locally Linear Embedding (LLE), ISOMAP, and Laplacian Eigenmaps (LE) seem like better candidates for dimensionality reduction of problems of this sort. For example, LE tries to preserve the local structure of the data while keeping the scale of the embedding data fixed, leading to good results on toy problems such as the swiss roll. We apply LLE and ISOMAP to this data set and show, rather counterintuitively, that the results are even worse than PCA.

\subsection{Nonlinear Methods}
Nonlinear methods, as demonstrated in the section below, are the most effective of the algorithms used for visualization of celllular protein data. Cellular protein data is largely believed to exist in multi-dimensional space in gaussian-distributed clumps that can have a high degree of overlap. These gaussians can exhibit high degrees of variation in their average and variance values. As such, we consider the variants of SNE \cite{Hinton:2002vs}, t-SNE \cite{VanderMaaten:2008tm} and s-SNE \cite{Cook:2007tb}, along with Elastic Embedding (EE) \cite{CarreiraPerpinan:2010tx} as prime candidates for effective visualization. These algorithms minimize objective functions that mimick attractive and repulsive forces in the mapping space, which keeps the images of nearby objects close while pushing all image clusters apart from each other. They take the form $E(X, \lambda) = E^+(X) + \lambda E^-(X)$ where $E^+(X)$ is the attractive term, $E^-(X)$ the repulsive term, $\lambda \geq 0$ is a fixed parameter, and $X = \{x_n\}$ is the set of points in the mapping space. The repulsive term significantly improves them over spectral methods like LE. 

%For comparison, we outline the objective functions $E$ of these algorithms.
%
%Given two neighborhood graphs,
%\begin{equation}
%\label{wnm+}
%w_{nm}^+ = exp(- \frac{1}{2} ||y_n - y_m||^2/\sigma^2) 
%\end{equation}
%
%\begin{equation}
%\label{wnm-}
%w_{nm}^- =||y_n - y_m||^2
%\end{equation}
%
%And conditional probability matrices $\{p_{nm}\}$ and $\{q_{nm}\}$ in both the original data space (points $y_n$) and mapping space (points $x_n$), respectively that a point selects any other point as its neighbor:
%
%\begin{equation}
%\label{pnm}
%p_{nm} = \frac{\exp(-||y_n - y_m||^2/2\sigma^2)}{\sum_{k \neq n} \exp(-||y_n - y_m||^2/2\sigma^2)}
%\end{equation}
%
%\begin{equation}
%\label{qnm}
%q_{nm} = \frac{\exp(-||x_n - x_m||^2/2\sigma^2)}{\sum_{k \neq n} \exp(-||x_n - x_m||^2/2\sigma^2)}
%\end{equation}
%
%The objective functions for LE, SNE, t-SNE, s-SNE, and EE are given by, 
%
%\begin{align*}
%E_{LE}(X) &= \sum_{n,m=1}^N w_{nm} ||x_n - x_m||^2 \\
%E_{SNE}(X) &= \sum_{n,m=1}^N p_{nm} ||x_n - x_m||^2 + \sum_{n=1}^N \log \sum_{m \neq n}^N \exp(-||x_n - x_m||^2) \\
%E_{s-SNE}(X) &= \sum_{n,m=1}^N p_{nm} ||x_n - x_m||^2 + \log \sum_{n,m =1}^N \exp(-||x_n - x_m||^2) \\
%E_{t-SNE}(X) &= \sum_{n,m=1}^N p_{nm} \log(1+||x_n - x_m||^2) \\ 
%	&+ \sum_{n,m =1}^N (1 + ||-x_n - x_m||^2)^{-1} \\
%E_{EE}(X) &= \sum_{n,m=1}^N w^+_{nm} ||x_n - x_m||^2 \\
%	&+ \lambda \sum_{n,m =1}^N w^-_{nm} \exp (-||x_n - x_m||^2)
%\end{align*}


\subsection{Fast Learning of Nonlinear Embeddings}
In order to speed up the SNE and EE algorithms, we employ a strategy for optimization of the objective functions in the "spectral direction" \cite{Vladymyrov:2012ws}. Optimization typically involves computing the gradient $g = \nabla E$ of the objective function and computing a descent direction as $p$ by solving the linear system $Bp = -g$ using some positive definitie $B$. When $B=I$ this is gradient descent, and for $B=\nabla^2 E$ we have Newton's Method. Descending in the spectral direction is accomplished by letting $B$ be the first term of $\nabla^2 E$ and, for small data sets ($<$ 10,000 points) is two orders of magnitude faster than gradient descent and Newton's Method.

\subsection{Metric on Quality of Visualization}
We quantify the quality of the various algorithms at accurately segregating different cell types in the map space using a simple re-labeling technique. After projection, the averages of all the points (centroids) of each cell sub-type are calculated. Each point is then relabeled based on whichever centroid it is closest to. Formally, for cell sub-type $i$ with $P^{[i]}$ points $\{x^{[i]}\}$ in the projection space, the centroid is given by $\mu^{[i]} = \frac{1}{P^{[i]}} \sum_{j} x_j^{[i]}$, and each point $x$ is relabeled as sub-type $i = \arg \min_j ||\mu^{[j]} - x||$. The error of a visualization with $N$ points is then calculated as $err = \frac{1}{N} \sum_j^N 1\{\text{originalLabel}(x_j) \neq \text{newLabel}(x_j) \}$.

% RESULTS
\section{Results}

\subsection{Linear Methods - PCA}
We consider PCA over all 20 cell sub-types with 400 cells per cell sub-type and obtain a 62.18\% re-labeling error. Figure ~\ref{pca_plots_allcells} below shows the visualized data. It is clear that the overlap between points renders the map meaningless. The colored stars in the figures represent the centroids of each sub-type.

\begin{figure}[h]
\vskip 0.2in
%\centering
\begin{center}
\subfloat[Original Labels]{\includegraphics[width=0.47\columnwidth, trim = 2.5in 0.5in 1in 2in]{PCA_AllCells_N=400}}\quad
\subfloat[New Labels]{\includegraphics[width=0.47 \columnwidth,  trim = 2.5in 0.5in 1in 2in]{PCA_AllCells_N=400_relabel}}\quad
\end{center}
\caption{Principal Components Analysis on all cell sub-types with 400 cells per subtype}
\label{pca_plots_allcells}
\vskip 0.2in
\end{figure}

Even when we reduce the number of sub-types to 7 (Monocytes and T cells), the performance is still abysmal at 48.43\% error, as shown in Figure ~\ref{pca_plots_monocytes&T}.

\begin{figure}[h]
\vskip 0.2in
%\centering
\begin{center}
\subfloat[Original Labels]{\includegraphics[width=0.47\columnwidth, trim = 2.5in 0.5in 1in 2in]{PCA_T&Moncytes_N=400}}\quad
\subfloat[New Labels]{\includegraphics[width=0.47 \columnwidth,  trim = 2.5in 0.5in 1in 2in]{PCA_T&Moncytes_N=400_relabel}}\quad
\end{center}
\caption{Principal Components Analysis on Monocytes and T Cells with 400 cells per subtype}
\label{pca_plots_monocytes&T}
\vskip -0.2in
\end{figure}

\subsection{Spectral Methods - ISOMAP}
We illustrate in Figure~\ref{isomaps} the spectral method ISOMAP on this data both for all cell sub-types (N/type = 100) and for just monocytes and T cells (N/type = 400) combined. In each case, the algorithm yields no discernible segmented high-level structure.

\begin{figure}[h]
\vskip 0.2in
%\centering
\begin{center}
\subfloat[All cell types]{\includegraphics[width=0.47\columnwidth, trim = 2.5in 0.5in 1in 2in]{ISOMAP_AllCells_N=100}}\quad
\subfloat[Monocytes and T Cells]{\includegraphics[width=0.47 \columnwidth,  trim = 2.5in 0.5in 1in 2in]{ISOMAP_Monoctypes&TCells_N=400}}\quad
\end{center}
\caption{ISOMAP with 400 cells per subtype}
\label{isomaps}
\vskip -0.2in
\end{figure}

\subsection{Nonlinear Methods}
We begin by considering t-SNE and sampling 400 cells from each cell type. We see in Figure~\ref{tSNEall} that the algorithm clusters the points reasonably well, even though it attains a relatively high error of 24.33\%. On the other hand, when we only work with Monocytes and T Cells (Figure~\ref{tSNEmonocytesandtcells}), we obtain a 7.46\% error - a 7-fold improvement on PCA for the same data set. 

Next, we consider s-SNE on a similar sample of 400 cells taken from either all cell types (Figure~\ref{sSNEall})  or  Monocytes and T Cells (Figure~\ref{sSNEmonocytesandtcells}). We see that the visualization appears to perform as well as tSNE, yet the metric shows that its performance is slightly worse. sSNE attained errors of 28.57\% and 8.18\% for all cell types and the subset of Monocytes and T Cells, respectively.  

We now look to EE, the last of the nonlinear methods we are considering, and run the algorithm with the same inputs as the previous methods. Once again, we take a sample of 400 cells from (i) all cell types and (ii) Monocytes and T Cells. Figure ~\ref{EEall} shows the visualization of all cell subtypes and Figure~\ref{EEmonocytesandtcells} shows it for Monocytes and T Cells. Though the algorithm works better than PCA, it performs substantially worse than the SNE variants with an error of 41.838\% for all cells and 12.46\% for just Monocytes and T Cells. In addition, there is less spacing between clusters than with the SNE variants.

Table~\ref{tableErrors} summarizes the errors for the different algorithms.
\begin{table}[h!]
\caption{Algorithm Errors}
\label{tableErrors}
%\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\hline
\abovespace\belowspace
Algorithm & All Cells & Monocytes \& T Cells \\
\hline
\abovespace
PCA	& 62.18\% & 48.42\%  \\
tSNE 	& 24.33\%  & 7.46\%\\ 
sSNE	& 28.67 \%  & 8.18\%\\
\belowspace
EE		& 41.84\% &12.46\% \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


% tSNE all
\begin{figure}[h!]
%\vskip 0.2in
\begin{center}
\subfloat[Original Labels]{\includegraphics[width=0.47\columnwidth, trim = 2.5in 0.5in 1in 2in]{tSNE_AllCells_N=400}}\quad
\subfloat[New Labels]{\includegraphics[width=0.47 \columnwidth,  trim = 2.5in 0.5in 1in 2in]{tSNE_AllCells_N=400_relabel}}\quad
\end{center}
\caption{tSNE on all cell sub-types with 400 cells per subtype}
\label{tSNEall}
\vskip -0.2in
\end{figure}

%tSNE monocytes and T
\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\subfloat[Original Labels]{\includegraphics[width=0.47\columnwidth, trim = 2.5in 0.5in 1in 2in]{tSNE_Monocytes&TCells_N=400}}\quad
\subfloat[New Labels]{\includegraphics[width=0.47 \columnwidth,  trim = 2.5in 0.5in 1in 2in]{tSNE_Monocytes&TCells_N=400_relabel}}\quad
\end{center}
\caption{tSNE on Monocytes and T Cells with 400 cells per subtype}
\label{tSNEmonocytesandtcells}
\vskip -0.2in
\end{figure}


%s-SNE for all cell types
\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\subfloat[Original Labels]{\includegraphics[width=0.47\columnwidth, trim = 2.5in 0.5in 1in 2in]{sSNE_AllCells_N=400}}\quad
\subfloat[New Labels]{\includegraphics[width=0.47 \columnwidth,  trim = 2.5in 0.5in 1in 2in]{sSNE_AllCells_N=400_relabel}}\quad
\end{center}
\caption{sSNE on all cell subtypes with 400 cells per subtype}
\label{sSNEall}
\vskip -0.2in
\end{figure}

%s-SNE for Monocytes and T Cells
\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\subfloat[Original Labels]{\includegraphics[width=0.47\columnwidth, trim = 2.5in 0.5in 1in 2in]{sSNE_Monocytes&TCells_N=400}}\quad
\subfloat[New Labels]{\includegraphics[width=0.47 \columnwidth,  trim = 2.5in 0.5in 1in 2in]{sSNE_Monocytes&TCells_N=400_relabel}}\quad
\end{center}
\caption{sSNE on Monocytes and T Cells with 400 cells per subtype}
\label{sSNEmonocytesandtcells}
\vskip -0.2in
\end{figure}


%EE for all cell types
\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\subfloat[Original Labels]{\includegraphics[width=0.47\columnwidth, trim = 2.5in 0.5in 1in 2in]{EE_AllCells_N=400}}\quad
\subfloat[New Labels]{\includegraphics[width=0.47 \columnwidth,  trim = 2.5in 0.5in 1in 2in]{EE_AllCells_N=400_relabel}}\quad
\end{center}
\caption{EE on all cell subtypes with 400 cells per subtype}
\label{EEall}
\vskip -0.2in
\end{figure}

% EE for monocytes and T Cells
\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\subfloat[Original Labels]{\includegraphics[width=0.47\columnwidth, trim = 2.5in 0.5in 1in 2in]{EE_Monocytes&TCells_N=400}}\quad
\subfloat[New Labels]{\includegraphics[width=0.47 \columnwidth,  trim = 2.5in 0.5in 1in 2in]{EE_Monocytes&TCells_N=400_relabel}}\quad
\end{center}
\caption{EE on Monocytes and T Cells with 400 cells per subtype}
\label{EEmonocytesandtcells}
\vskip -0.2in
\end{figure}

\subsection{Classification}
Shifting our attention from visualization to classification, we compare the classification effectiveness of C-SVC and $\nu$-SVC SVM (per the LIBSVM library \cite{Chang:2011dt}) run on the original data to the projected data, and reach a counter-intuitive result. The SVM applied to the original data fails entirely, with low success rates. The SVM applied to the projected data classifies with significantly higher accuracies, depending on the dimensionality reduction algorithm used. Table~\ref{svmrates} shows the success rates of C-SVC and $\nu$-SVC with various kernels (with s-SNE omitted as its values are very close to t-SNE). We find that the best classification is achieved, with a success rate of 96.2\%, when using $\nu$-SVC based SVM with a polynomial kernel. 

\begin{table}[]
\caption{SVM Classification Rate }
\label{svmrates}
%\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}

\begin{tabular}{lcccr}
\hline
\abovespace\belowspace
C-SVC 		& Original 	& PCA	 & t-SNE 		 & EE  \\
\hline
\abovespace
radial 		& 5.2\% 	& 91.3\% 	& 78.3 \% 	 	& 59.1 \% \\
polyn. 		& 4.6\%  	& 92.3\% 	& 74.6 \% 		& 55.6 \% \\ 
linear 		& 5.53 \%  & 93.3\% & 74.07\% 	 	& 56.9 \% \\
\belowspace
sigmoid		& 5.8\% 	& 70.8\% 	& 8.4 \% 	 	& 7.6 \% \\
\hline
\hline
\abovespace\belowspace
nu-SVC 		& Original 	& PCA	 & t-SNE 		 & EE  \\
\hline
\abovespace
radial 		& 5\% 	& 90\% 	& 76.4 \% 	 	& 58 \% \\
polyn. 		& 4.4\%  	& 96.2\% 	& 66.8 \% 		& 51.2 \% \\ 
linear 		& 5.8 \%  	& 91.4\% 	& 68.2\% 	 	& 46.8 \% \\
\belowspace
sigmoid		& 4.6\% 	& 71\% 	& 26.4 \% 	 	& 9.8 \% \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table}

%\begin{table}[h!]
%\caption{SVM Classification Rate }
%\label{svmrates}
%%\vskip 0.15in
%\begin{center}
%\begin{small}
%\begin{sc}
%
%\begin{tabular}{lcccr}
%\hline
%\abovespace\belowspace
%C-SVC 		& Original & PCA & t-SNE 	& s-SNE & EE  \\
%\hline
%\abovespace
%radial 	& 5.2\% 	& 91.27\% 	& 78.3 \% 	& 76.47 \% 	& 59.13 \% \\
%polyn. 	& 4.6\%  	& 92.3\% 	& 74.6 \% 	& 72.1 \% 		& 55.6 \% \\ 
%linear 		& 5.53 \%  & 93.27\% & 74.07\% 	& 72.67 \% 	& 56.93 \% \\
%\belowspace
%sigmoid		& 5.8\% 	& 70.8\% 	& 8.4 \% 	& 9.6 \% 		& 7.6 \% \\
%\hline
%\end{tabular}
%\end{sc}
%\end{small}
%\end{center}
%\vskip -0.1in
%\end{table}


%\subsection{Citations and References} 
%
%Please use APA reference format regardless of your formatter
%or word processor. If you rely on the \LaTeX\/ bibliographic 
%facility, use {\tt natbib.sty} and {\tt icml2012.bst} 
%included in the style-file package to obtain this format.
%
%Citations within the text should include the authors' last names and
%year. If the authors' names are included in the sentence, place only
%the year in parentheses, for example when referencing Arthur Samuel's
%pioneering work \yrcite{Bendall:2011bm}. Otherwise place the entire
%reference in parentheses with the authors and year separated by a
%comma \cite{Bendall:2011bm}. List multiple references separated by
%semicolons \cite{Bendall:2011bm, Crammer:2002uy, Bendall:2011bm}. Use the `et~al.'
%construct only for citations with three or more authors or after
%listing all authors to a publication in an earlier reference \cite{Bendall:2011bm}.
%
%Authors should cite their own work in the third person
%in the initial version of their paper submitted for blind review.
%Please refer to Section~\ref{author info} for detailed instructions on how to
%cite your own papers.
%
%Use an unnumbered first-level section heading for the references, and 
%use a hanging indent style, with the first line of the reference flush
%against the left margin and subsequent lines indented by 10 points. 
%The references at the end of this document give examples for journal
%articles \cite{Bendall:2011bm}, conference publications \cite{Bendall:2011bm}, book chapters \cite{Bendall:2011bm}, books \cite{Bendall:2011bm}, edited volumes \cite{Bendall:2011bm}, 
%technical reports \cite{Crammer:2002uy}, and dissertations \cite{Bendall:2011bm}. 
%
%Alphabetize references by the surnames of the first authors, with
%single author entries preceding multiple author entries. Order
%references for the same authors by year of publication, with the
%earliest first. Make sure that each reference includes all relevant
%information (e.g., page numbers).

%\cite{Hinton:2002vs}
%\cite{Nam:2004jm}
%\cite{CarreiraPerpinan:2010tx}
%\cite{VanderMaaten:2008tm}
%\cite{Cook:2007tb}
%\cite{Bendall:2011bm}

% Acknowledgements should only appear in the accepted version. 
\section{Conclusion} 

Human immune system cells are comprised of a variety of cell types and sub-types which are characterized by the proteins contained within them. Proteins embedded on the cell's surface are particularly effective at characterizing the type of cell, and through 'gating', biologists can label a cell's type from these proteins by plotting various permutations of two types of proteins at time. We apply dimensionality reduction considering only the surface proteins as features to aid the visualization of cells in this multi-dimensional protein space. We apply Principle Components Analysis, ISOMAP, elastic embedding, symmetric-stochastic neighbor embedding, and student t-distributed stochastic neighbor embedding to this problem and use a centroid-based relabeling metric to quantify the effectiveness of each visualization. We demonstrate the superiority of the nonlinear methods, with t-SNE being the most effective, while simultaneously showing that PCA and ISOMAP yield poor results. We further conclude that the data is nonlinear and that it supports the belief that clusters of healthy cells in this protein space are gaussian-distributed. 

In addition, we demonstrate a substantial improvement in SVM-based classification of data by first reducing the dimensionality of the data and then applying classification on the reduced data. We find that by combining PCA with $\nu$-SVC based SVM and a polynomial kernel a 96.2\% classification rate is achieved. 

Our work provides a general tool which biologists can use to visualize and better understand multi-dimensional cellular protein data. This is a first step towards automated cell-labelling and further work could yield a scalable platform by which both healthy and unhealthy immune system cells could be easily categorized, leading to a cancerous-cell detection system. Additionally this can be used by researchers to track how immune system cells' protein content changes as they mature, evolve, and differentiate. 

%Human immune system cells are comprised of a variety of cell types and sub-types. We choose 17 surface protein expressions to define each cell. This is too much to visualize for the biologists. Thus, we propose various linear and nonlinear methods for dimensionality reduction of the original multi-dimensional data so that it can be visualized by cancer immunologists easily. We run these methods on 400 samples of different cell types - pDC, monocytes, B-Cells and T-Cells. We then compare all the methods and see which method helps segregate the various cell types the best. The linear methods such as Principle Component Analysis (PCA) do not seem to help in clustering of data very well leading us to believe that the data is not linearly separable. The data was found to be easily modeled and segregated by non-linear methods. Stochastic Neighbour Embedding (SNE) and its variants (t-SNE and v-SNE) seem to work best on the data. Stochastic Neighbour Embedding (SNE) and its variants (t-SNE and s-SNE) are found to be the best for the data. t-SNE is the best among all the other nonlinear methods as seen from the plots. This is because it uses a Student distribution versus a Gaussian distribution and t-Student distribution seems to model the data really well. 

%Cancer immunologists can look at the projection of multidimensional data in 2 Dimensional map space. The 2D visualizations of the various cell types provides the immunologists with an easy way to track the shifting of their representation. Biologists can also use these algorithms on unlabelled data for prediction classification.  


% Acknowledgements should only appear in the accepted version. 
\section{Acknowledgments} 

We would like to thank Karen Sachs (Stanford University) and Andrew Gentes (Stanford University) for their support and guidance throughout this project, as well as for providing the data required for its completion. 


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{Bendall:2011bm}

{\footnotesize
\bibliography{Final_Report_bib2}}
%\bibliography{Final_Report_bib2}
%%\bibliographystyle{icml2012}
\bibliographystyle{naturemag}

%\begin{thebibliography}{1}
%\bibitem{Hinton:2002vs} Hinton, G. E. & Roweis, S. T. 1. Hinton, G. E. & Roweis, S. T. Stochastic neighbor embedding. Advances in neural information (2002)
%\bibitem{Nam:2004jm} Nam, K., Je, H. & Choi, S. Fast stochastic neighbor embedding: a trust-region algorithm. IEEE Joint Conference on Neural Networks, (2004), 
%\bibitem{Crammer:2002uy} Crammer, K. & Singer, Y. On the algorithmic implementation of multiclass kernel-based vector machines. The Journal of Machine Learning Research 2, 265–292 (2002).
%\bibitem{dummy} Articles are restricted to 50 references, Letters
%to 30.
%\bibitem{dummyb} No compound references -- only one source per
%reference.
%\cite{Vladymyrov:2012ws} Vladymyrov, M. & Carreira-Perpinan, M. Partial-Hessian strategies for fast learning of nonlinear embeddings. arXiv preprint arXiv:1206.4646 (2012).
%\cite{Bendall:2011bm} Bendall, S. C. et al. Single-Cell Mass Cytometry of Differential Immune and Drug Responses Across a Human Hematopoietic Continuum. Science 332, 687–696 (2011).
%\end{thebibliography}


\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  


